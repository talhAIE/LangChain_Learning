{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a36a36",
   "metadata": {},
   "source": [
    "# Langchain Retrivers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d9c8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "retrieverwiki=WikipediaRetriever(top_k_results=2,lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ee2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Attention is all you need\"\n",
    "docs=retrieverwiki.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee32667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Attention Is All You Need', 'summary': '\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers franchise. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2025, the paper has been cited more than 173,000 times, placing it among top ten most-cited papers of the 21st century.', 'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need'}, page_content='\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers franchise. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2025, the paper has been cited more than 173,000 times, placing it among top ten most-cited papers of the 21st century.\\n\\n\\n== Authors ==\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group\\'s diversity:\\nSix of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\\n\\nAfter the paper, each of the authors left Google to join other companies or to found startups. Several of them expressed feelings of being unable to innovate and expand the Transformer in a direction they want, if they had stayed at Google.\\n\\n\\n== Methods discussed and introduced ==\\nThe paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\\nThe following mechanisms were introduced by the paper as part of the development of the transformer architecture.\\nScaled dot-product attention & self-attention\\nThe use of the scaled dot-product attention and self-attention mechanism instead of a Recurrent neural network or Long short-term memory (which rely on recurrence instead) allow for better performance as described in the following paragraph. The paper described the scaled dot-product attention as follows:\\n\\n  \\n    \\n      \\n        \\n          \\n            A\\n            t\\n            t\\n            e\\n            n\\n            t\\n            i\\n            o\\n            n\\n          \\n        \\n        (\\n        Q\\n        ,\\n        K\\n        ,\\n        V\\n        )\\n        :=\\n        \\n          \\n            s\\n            o\\n            f\\n            t\\n            m\\n            a\\n            x\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                Q\\n                ×\\n                \\n                  K\\n                  \\n                    T\\n               '),\n",
       " Document(metadata={'title': 'All You Need Is Kill', 'summary': \"All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Ryosuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha's Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A live-action film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title. An anime film adaptation produced by Studio 4°C has been announced.\\nThe novel was Sakurazaka's breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chōhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\", 'source': 'https://en.wikipedia.org/wiki/All_You_Need_Is_Kill'}, page_content='All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Ryosuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha\\'s Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A live-action film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title. An anime film adaptation produced by Studio 4°C has been announced.\\nThe novel was Sakurazaka\\'s breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chōhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\\n\\n\\n== Plot ==\\nThe story is told from the perspective of Keiji Kiriya, a new recruit in the United Defense Force. Despite equipping its soldiers with powered exoskeletons, the UDF is losing its fight against the mysterious \"Mimics\" (ギタイ, gitai), extraterrestrials who have laid siege to Earth. Keiji is seemingly killed on his first sortie after killing an unusual-looking Mimic but, through some inexplicable phenomenon, he wakes and finds that he has returned to the day before the battle. As this process continues, he finds himself caught in a time loop as his death and resurrection repeat time and time again. Keiji\\'s skill as a soldier grows as he passes through each time loop in a desperate attempt to change his fate. After several dozen loops, he realizes his fate is similar to that of Rita Vrataski, a prominent ace who preferred to use a battle axe rather than a firearm. He uses his knowledge of the day to get close to her and her mechanic, from whom he gets a copy of her massive axe. He learns to use the weapon well; the boltgun that most troops are issued quickly runs out of ammo and jams easily.\\nRealizing that he is a fellow looper, Rita confides in Keiji, telling him of the system the Mimics use: on death, they have the ability to send a signal into the past, allowing them to see the future and change their behavior to avoid that fate. In each group of Mimics, there is one that acts as a central nexus that can cause the day to loop, as well as several antenna Mimics, all of which signal the loop to reset; Keiji became trapped in the loop as a result of contact with one such antenna. To escape as Rita once did, Keiji must first kill all the antennae and then the nexus. The Mimics constantly adapt to Keiji\\'s attacks. He and Rita manage to eliminate the nexus, only to have the loop reset with Rita forgetting what has transpired. After telling Rita this, she acknowledges that they missed one antenna. On the 160th loop, they proceed to eliminate the antennae again. Rita then attacks Keiji once they are out of sight of allied forces, explaining her hypothesis that being trapped in the loop has modified their brains. In essence, both of them are similar to the antenna Mimics, meaning one of them has to die before killing the nexus; otherwise, the loop will continue indefinitely. Reluctantly, the two battle. Keiji mortally wounds Rita and stays by her side as she dies. Before Rita dies, Keiji confesses his d')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe2efe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers franchise. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2025, the paper has been cited more than 173,000 times, placing it among top ten most-cited papers of the 21st century.\\n\\n\\n== Authors ==\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group\\'s diversity:\\nSix of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\\n\\nAfter the paper, each of the authors left Google to join other companies or to found startups. Several of them expressed feelings of being unable to innovate and expand the Transformer in a direction they want, if they had stayed at Google.\\n\\n\\n== Methods discussed and introduced ==\\nThe paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\\nThe following mechanisms were introduced by the paper as part of the development of the transformer architecture.\\nScaled dot-product attention & self-attention\\nThe use of the scaled dot-product attention and self-attention mechanism instead of a Recurrent neural network or Long short-term memory (which rely on recurrence instead) allow for better performance as described in the following paragraph. The paper described the scaled dot-product attention as follows:\\n\\n  \\n    \\n      \\n        \\n          \\n            A\\n            t\\n            t\\n            e\\n            n\\n            t\\n            i\\n            o\\n            n\\n          \\n        \\n        (\\n        Q\\n        ,\\n        K\\n        ,\\n        V\\n        )\\n        :=\\n        \\n          \\n            s\\n            o\\n            f\\n            t\\n            m\\n            a\\n            x\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                Q\\n                ×\\n                \\n                  K\\n                  \\n                    T\\n               '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c6607",
   "metadata": {},
   "source": [
    "# Maximal Marginal Relavance (MMR)\n",
    "\n",
    "to get diverse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fad34c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "doc = [\n",
    "    Document(page_content=\"Agentic AI refers to artificial intelligence systems that can operate autonomously, make decisions, and take actions to achieve specific goals. Unlike traditional AI models that passively respond to prompts, agentic systems actively plan, reason, and interact with their environment. These systems combine LLMs with tools, memory, and multi-step workflows to complete tasks with minimal human input.\"),\n",
    "    \n",
    "    Document(page_content=\"One of the key features of agentic AI is its ability to break down complex tasks into smaller subtasks. Using reasoning capabilities and access to APIs, databases, or other tools, an agent can decide what actions to take and in what order. For example, a research assistant agent can read documents, summarize key points, and generate a report without being micromanaged.\"),\n",
    "    \n",
    "    Document(page_content=\"LangChain provides the infrastructure to build agentic AI applications by combining language models with tools, memory, and agents. Agents in LangChain use a chain of thought approach, where the LLM generates intermediate steps and decides which tools to use next. This makes it ideal for applications like automated customer support, market research, and workflow automation.\"),\n",
    "    \n",
    "    Document(page_content=\"Agentic AI is widely used in productivity tools such as smart email assistants, coding copilots, and task managers. These agents don't just respond — they reason, schedule tasks, write emails, and even manage projects by interacting with multiple apps. This level of autonomy can save professionals hours of repetitive work every week.\"),\n",
    "    \n",
    "    Document(page_content=\"Despite its power, agentic AI poses challenges such as reliability, safety, and alignment with human intent. Agents may take incorrect actions if not properly constrained or monitored. Developers must implement safeguards, reasoning checks, and fallback strategies to ensure agentic systems behave responsibly and transparently.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c508547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embeddings=OpenAIEmbeddings()\n",
    "vectorstore=FAISS.from_documents(\n",
    "    doc,embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d8be4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable MMR in the retriver to get diverse results\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type='mmr', # enable retriver MMR\n",
    "    search_kwargs={\"k\":3, \"lambda_mult\" :0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d49527",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"key features of agentic Ai\"\n",
    "results=retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bd74b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "One of the key features of agentic AI is its ability to break down complex tasks into smaller subtasks. Using reasoning capabilities and access to APIs, databases, or other tools, an agent can decide what actions to take and in what order. For example, a research assistant agent can read documents, summarize key points, and generate a report without being micromanaged.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "LangChain provides the infrastructure to build agentic AI applications by combining language models with tools, memory, and agents. Agents in LangChain use a chain of thought approach, where the LLM generates intermediate steps and decides which tools to use next. This makes it ideal for applications like automated customer support, market research, and workflow automation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Despite its power, agentic AI poses challenges such as reliability, safety, and alignment with human intent. Agents may take incorrect actions if not properly constrained or monitored. Developers must implement safeguards, reasoning checks, and fallback strategies to ensure agentic systems behave responsibly and transparently.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i , doc in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ba6909",
   "metadata": {},
   "source": [
    "# Multi-Query Retriver \n",
    "\n",
    "Solve Ambuiguity by generating multiple queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0711bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Relevant health & wellness documents\n",
    "all_docs = [\n",
    "    Document(page_content=\"Regular walking boosts heart health and can reduce symptoms of depression.\", metadata={\"source\": \"H1\"}),\n",
    "    Document(page_content=\"Consuming leafy greens and fruits helps detox the body and improve longevity.\", metadata={\"source\": \"H2\"}),\n",
    "    Document(page_content=\"Deep sleep is crucial for cellular repair and emotional regulation.\", metadata={\"source\": \"H3\"}),\n",
    "    Document(page_content=\"Mindfulness and controlled breathing lower cortisol and improve mental clarity.\", metadata={\"source\": \"H4\"}),\n",
    "    Document(page_content=\"Drinking sufficient water throughout the day helps maintain metabolism and energy.\", metadata={\"source\": \"H5\"}),\n",
    "    Document(page_content=\"The solar energy system in modern homes helps balance electricity demand.\", metadata={\"source\": \"I1\"}),\n",
    "    Document(page_content=\"Python balances readability with power, making it a popular system design language.\", metadata={\"source\": \"I2\"}),\n",
    "    Document(page_content=\"Photosynthesis enables plants to produce energy by converting sunlight.\", metadata={\"source\": \"I3\"}),\n",
    "    Document(page_content=\"The 2022 FIFA World Cup was held in Qatar and drew global energy and excitement.\", metadata={\"source\": \"I4\"}),\n",
    "    Document(page_content=\"Black holes bend spacetime and store immense gravitational energy.\", metadata={\"source\": \"I5\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87f86705",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2=FAISS.from_documents(all_docs,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dbcd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_retriver=vectorstore2.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0613a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_retriever=MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore2.as_retriever(search_kwargs={\"k\":5}),\n",
    "    llm=ChatOpenAI(model=\"gpt-4o\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a24cd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"how to improve energy level and maintain balance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92ec601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive results\n",
    "similarity_results=similarity_retriver.invoke(query)\n",
    "multiquery_results=multiquery_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "065a2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Drinking sufficient water throughout the day helps maintain metabolism and energy.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "Mindfulness and controlled breathing lower cortisol and improve mental clarity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Consuming leafy greens and fruits helps detox the body and improve longevity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4:\n",
      "Regular walking boosts heart health and can reduce symptoms of depression.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5:\n",
      "Deep sleep is crucial for cellular repair and emotional regulation.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(similarity_results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62b7753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Drinking sufficient water throughout the day helps maintain metabolism and energy.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "Consuming leafy greens and fruits helps detox the body and improve longevity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Mindfulness and controlled breathing lower cortisol and improve mental clarity.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 4:\n",
      "The solar energy system in modern homes helps balance electricity demand.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 5:\n",
      "Deep sleep is crucial for cellular repair and emotional regulation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 6:\n",
      "Regular walking boosts heart health and can reduce symptoms of depression.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(multiquery_results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe20cb7",
   "metadata": {},
   "source": [
    "# contextual Compression Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "674b070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the document objects from the previous data\n",
    "docs1 = [\n",
    "    Document(page_content=(\n",
    "        \"\"\"The Grand Canyon is one of the most visited natural wonders in the world.\n",
    "        Photosynthesis is the process by which green plants convert sunlight into energy.\n",
    "        Millions of tourists travel to see it every year. The rocks date back millions of years.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc1\"}),\n",
    "\n",
    "    Document(page_content=(\n",
    "        \"\"\"In medieval Europe, castles were built primarily for defense.\n",
    "        The chlorophyll in plant cells captures sunlight during photosynthesis.\n",
    "        Knights wore armor made of metal. Siege weapons were often used to breach castle walls.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc2\"}),\n",
    "\n",
    "    Document(page_content=(\n",
    "        \"\"\"Basketball was invented by Dr. James Naismith in the late 19th century.\n",
    "        It was originally played with a soccer ball and peach baskets. NBA is now a global league.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc3\"}),\n",
    "\n",
    "    Document(page_content=(\n",
    "        \"\"\"The history of cinema began in the late 1800s. Silent films were the earliest form.\n",
    "        Thomas Edison was among the pioneers. Photosynthesis does not occur in animal cells.\n",
    "        Modern filmmaking involves complex CGI and sound design.\"\"\"\n",
    "    ), metadata={\"source\": \"Doc4\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac0852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f8785bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore4=FAISS.from_documents(docs1,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cada7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever=vectorstore4.as_retriever(search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "54fe1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the compressor using an LLM\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "compressor=LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f17720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriver compressoin\n",
    "compression_retriever=ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38a123e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3=\"What is photosynthesis?\"\n",
    "compressed_results=compression_retriever.invoke(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba221c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Photosynthesis is the process by which green plants convert sunlight into energy.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "The chlorophyll in plant cells captures sunlight during photosynthesis.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Photosynthesis does not occur in animal cells.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i , doc in enumerate(compressed_results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
